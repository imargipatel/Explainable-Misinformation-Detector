{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1CYDRMpYvmxJY43yHtTmf9WoY4W5UGmOG",
      "authorship_tag": "ABX9TyNgWRPrG47kr4EJJLKaT7rE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imargipatel/Explainable-Misinformation-Detector/blob/main/NLP_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation and Setup\n"
      ],
      "metadata": {
        "id": "qG3EesGT4wwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Core NLP Linraries\n",
        "!pip install transformers torch scikit-learn scipy\n",
        "!pip install spacy textblob\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "sz35gC3_Bzse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data handling\n",
        "!pip install pandas numpy -q\n",
        "!pip install nltk -q"
      ],
      "metadata": {
        "id": "aKTTEGAFCBeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization (optional, for later)\n",
        "!pip install matplotlib seaborn plotly -q"
      ],
      "metadata": {
        "id": "UWTyq61uCVC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download FEVER dataset\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "FU5BXt_tCaVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standard libraries\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9-D-CR0a5bqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk.corpus\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n"
      ],
      "metadata": {
        "id": "CCNZLSC35u6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "u1WCxKRL6bw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "2FvanEiL652-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "nltk.download('universal_tagset', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) # Added to fix the LookupError"
      ],
      "metadata": {
        "id": "-Y4d5SNT7BQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "tyfPWTVa7o4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 10"
      ],
      "metadata": {
        "id": "6gH8BtAY7w46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
      ],
      "metadata": {
        "id": "bUhpMWfFCe2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading FEVER Dataset"
      ],
      "metadata": {
        "id": "GoztAs64ColW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==2.19.0"
      ],
      "metadata": {
        "id": "nYWco3LZHV4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "fever = load_dataset(\"fever\", \"v1.0\", trust_remote_code=True)\n",
        "print(fever)\n",
        "\n"
      ],
      "metadata": {
        "id": "43t6IDh8PnpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract splits\n",
        "train_data = fever['train']\n",
        "dev_data = fever['paper_dev']\n",
        "test_data = fever['paper_test']"
      ],
      "metadata": {
        "id": "RybchTNdRbwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to pandas for easier handling\n",
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_data)\n",
        "dev_df = pd.DataFrame(dev_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "print(f\"Training: {len(train_df)} | Dev: {len(dev_df)} | Test: {len(test_df)}\")\n",
        "print(f\"Columns: {train_df.columns.tolist()}\")"
      ],
      "metadata": {
        "id": "KfAGJjq2TApX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training set: {len(train_df)} claims\")\n",
        "print(f\"Dev set: {len(dev_df)} claims\")\n",
        "print(f\"Test set: {len(test_df)} claims\")\n",
        "print(f\"Total: {len(train_df) + len(dev_df) + len(test_df)} claims\")"
      ],
      "metadata": {
        "id": "WW5b-Ljr8WQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n Dataset columns: {train_df.columns.tolist()}\")\n",
        "print(f\"\\n Sample row stucture: {train_df.iloc[0]}\")\n"
      ],
      "metadata": {
        "id": "JriH6I1y84I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "rxj3Sf539RlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    #Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    #Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    #Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    #Remove special characters but keep necessary punctuation\n",
        "    text = re.sub(r'[^\\w\\s\\.\\!\\?ulations\\-\\'\\\"\\;]', '', text)\n",
        "\n",
        "    #Remove multiple punctuation\n",
        "    text = re.sub(r'\\.{2,}|!{2,}|\\?{2,}', lambda m: m.group(0)[0], text)\n",
        "\n",
        "    #Remove extra spaces\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "LkXjMlDi9Wj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import critical\n",
        "def preprocess_dataframe(df, name=\"Dataset\"):\n",
        "    \"\"\"Apply all preprocessing steps to a dataframe\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    print(f\"\\nProcessing {name}:\")\n",
        "    initial_count = len(df)\n",
        "\n",
        "    # Drop rows with missing critical columns\n",
        "    critical_cols = ['claim', 'label'] if 'label' in df.columns else ['claim']\n",
        "    df = df.dropna(subset=critical_cols)\n",
        "    dropped = initial_count - len(df)\n",
        "    if dropped > 0:\n",
        "        print(f\"  • Dropped {dropped} rows with missing values\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    dup_count = df.duplicated(subset=['claim']).sum()\n",
        "    df = df.drop_duplicates(subset=['claim'], keep='first')\n",
        "    if dup_count > 0:\n",
        "        print(f\"  • Removed {dup_count} duplicate claims\")\n",
        "\n",
        "    # Clean claims\n",
        "    df['claim_clean'] = df['claim'].apply(clean_text)\n",
        "\n",
        "    # Remove empty claims\n",
        "    empty_count = (df['claim_clean'].str.len() == 0).sum()\n",
        "    df = df[df['claim_clean'].str.len() > 0]\n",
        "    if empty_count > 0:\n",
        "        print(f\"  • Removed {empty_count} empty claims after cleaning\")\n",
        "\n",
        "    # Add basic text features\n",
        "    df['claim_length'] = df['claim_clean'].str.split().str.len()\n",
        "    df['claim_char_length'] = df['claim_clean'].str.len()\n",
        "    df['num_sentences'] = df['claim_clean'].apply(lambda x: len(sent_tokenize(x)))\n",
        "\n",
        "    print(f\"  Final size: {len(df)} claims\")\n",
        "    print(f\"  Claim length - Mean: {df['claim_length'].mean():.2f}, Median: {df['claim_length'].median():.0f}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "STtrDzqk_D--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess all the datasets\n",
        "train_df = preprocess_dataframe(train_df, \"Train Set\")\n",
        "dev_df = preprocess_dataframe(dev_df, \"Dev Set\")\n",
        "test_df = preprocess_dataframe(test_df, \"Test Set\")"
      ],
      "metadata": {
        "id": "R7v7JN9GD6gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "RmDigpGMTtF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label distribution\n",
        "print(\"\\nLABEL DISTRIBUTION\")\n",
        "for name, df in [(\"Train\", train_df), (\"Dev\", dev_df)]:\n",
        "    print(f\"\\n{name} Set:\")\n",
        "    label_dist = df['label'].value_counts().sort_index()\n",
        "    for label, count in label_dist.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        label_names = {0: \"SUPPORTED\", 1: \"REFUTED\", 2: \"NOT_ENOUGH_INFO\"}\n",
        "        print(f\"  • {label_names.get(label, label)}: {count:,} ({pct:.1f}%)\")"
      ],
      "metadata": {
        "id": "Dq3ER8CdTz4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Claim statistics\n",
        "print(\"\\nCLAIM STATISTICS\")\n",
        "for name, df in [(\"Train\", train_df), (\"Dev\", dev_df), (\"Test\", test_df)]:\n",
        "    print(f\"\\n{name} Set:\")\n",
        "    print(f\"  • Claim length (words):\")\n",
        "    print(f\"    - Mean: {df['claim_length'].mean():.2f}\")\n",
        "    print(f\"    - Median: {df['claim_length'].median():.0f}\")\n",
        "    print(f\"    - Std: {df['claim_length'].std():.2f}\")\n",
        "    print(f\"    - Min-Max: {df['claim_length'].min():.0f}-{df['claim_length'].max():.0f}\")\n",
        "    print(f\"  Number of sentences - Mean: {df['num_sentences'].mean():.2f}\")"
      ],
      "metadata": {
        "id": "05vW1F7cT6Uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Missing values check\n",
        "print(\"\\n MISSING VALUES CHECK \")\n",
        "for name, df in [(\"Train\", train_df), (\"Dev\", dev_df), (\"Test\", test_df)]:\n",
        "    print(f\"\\n{name} Set:\")\n",
        "    missing = df.isnull().sum()\n",
        "    if missing.sum() == 0:\n",
        "        print(\"No missing values\")\n",
        "    else:\n",
        "        for col, count in missing[missing > 0].items():\n",
        "            print(f\"  • {col}: {count} ({count/len(df)*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "M6waX7kKFKVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample claims\n",
        "print(\"\\nSample Claims by Label:\")\n",
        "for label in train_df['label'].unique():\n",
        "    sample = train_df[train_df['label'] == label]['claim'].iloc[0]\n",
        "    print(f\"Label {label}: {sample}\")"
      ],
      "metadata": {
        "id": "j9MjKrwUUKR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization\n"
      ],
      "metadata": {
        "id": "QUYW6-S0Fc35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Label Distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "label_names = {0: \"SUPPORTED\", 1: \"REFUTED\", 2: \"NOT_ENOUGH_INFO\"}\n",
        "\n",
        "for idx, (name, df) in enumerate([(\"Train\", train_df), (\"Dev\", dev_df)]):\n",
        "    counts = df['label'].value_counts().sort_index()\n",
        "    axes[idx].bar(counts.index, counts.values, color=['#2ecc71', '#e74c3c', '#95a5a6'])\n",
        "    axes[idx].set_title(f'{name} - Label Distribution')\n",
        "    axes[idx].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('01_labels.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xq-Z-VZ2VIQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Claim Length Distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "for idx, (name, df) in enumerate([(\"Train\", train_df), (\"Dev\", dev_df)]):\n",
        "    df['claim_length'] = df['claim_clean'].str.split().str.len()\n",
        "    axes[idx].hist(df['claim_length'], bins=30, color='#3498db', edgecolor='black')\n",
        "    axes[idx].axvline(df['claim_length'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"claim_length\"].mean():.1f}')\n",
        "    axes[idx].set_title(f'{name} - Claim Length')\n",
        "    axes[idx].set_xlabel('Words')\n",
        "    axes[idx].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('02_length.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qFX0b0GXVWLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracted Linguistic Features"
      ],
      "metadata": {
        "id": "FqN0eK08S7dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tbl import feature\n",
        "from textblob.en import polarity\n",
        "def extract_features(text):\n",
        "\n",
        "  if not text or pd.isna(text):\n",
        "    return {f: 0 for f in ['num_words', 'num_sentences','num_adj','num_adv','num_modal','polarity','subjectivity']}\n",
        "\n",
        "  text = str(text)\n",
        "  words = word_tokenize(text)\n",
        "  pos_tags = nltk.pos_tag(words, tagset='universal')\n",
        "\n",
        "  #Count Features\n",
        "  num_words = len(words)\n",
        "  num_sentences = len(sent_tokenize(text))\n",
        "  num_adj = sum(1 for _, tag in pos_tags if tag == 'ADJ')\n",
        "  num_adv = sum(1 for _, tag in pos_tags if tag == 'ADV')\n",
        "\n",
        "  #Modal Verbs\n",
        "  modals = {'can', 'could', 'may', 'might', 'must', 'should','will', 'would'}\n",
        "  num_modal = sum(1 for word in words if word.lower() in modals)\n",
        "\n",
        "  #Sentiment\n",
        "  blob = TextBlob(text)\n",
        "  polarity = blob.sentiment.polarity\n",
        "  subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "  return {\n",
        "      'num_words': num_words,\n",
        "      'num_sentences': num_sentences,\n",
        "      'num_adj': num_adj,\n",
        "      'num_adv': num_adv,\n",
        "      'num_modal': num_modal,\n",
        "      'polarity': polarity,\n",
        "      'subjectivity': subjectivity\n",
        "  }"
      ],
      "metadata": {
        "id": "va68euyaTHCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraxt from sample\n",
        "print(\"Extracting features from sample:\")\n",
        "\n",
        "sample_size = min(2000, len(train_df))\n",
        "features_list =[]\n",
        "for i in range(sample_size):\n",
        "    claim_text = train_df.iloc[i]['claim_clean']\n",
        "    claim_label = train_df.iloc[i]['label'] # Get the label from train_df\n",
        "    feat = extract_features(claim_text)\n",
        "    feat['label'] = claim_label # Add label to the feature dictionary\n",
        "    features_list.append(feat)\n",
        "\n",
        "features_df = pd.DataFrame(features_list)\n",
        "print(f\"Features extracted from {len(features_df)} claims\")\n",
        "print(f\"\\n Features Statistics:\")\n",
        "print(features_df.describe())"
      ],
      "metadata": {
        "id": "86YQ5ki6WiIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These bar charts will display the average value of each linguistic feature for each claim label (SUPPORTS, REFUTES, NOT ENOUGH INFO), along with error bars to show the variability around that average.\n",
        "\n",
        "This provides a very clear and intuitive way to compare how these features differ across the claim types.\n",
        "\n",
        "Comparing Features by Label: These plots clearly show the average value of each feature (like num_words, polarity, subjectivity, etc.) for 'SUPPORTS', 'REFUTES', and 'NOT ENOUGH INFO' claims.\n",
        "\n",
        "Ease of Interpretation: This format makes it very straightforward to see at a glance if one label typically has, for example, more words, higher subjectivity, or a different sentiment than another. The error bars (showing standard deviation) give you an idea of how much variation there is around that average."
      ],
      "metadata": {
        "id": "OT_XAHgKairx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb104635"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Map original labels to more descriptive names for plotting clarity\n",
        "plot_label_names = {\"SUPPORTS\": \"SUPPORTS\", \"REFUTES\": \"REFUTES\", \"NOT ENOUGH INFO\": \"NOT ENOUGH INFO\"}\n",
        "features_df['label_display'] = features_df['label'].map(plot_label_names)\n",
        "\n",
        "features_to_plot = ['num_words', 'num_sentences', 'num_adj', 'num_adv', 'num_modal', 'polarity', 'subjectivity']\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10)) # Adjusted subplot grid\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(features_to_plot):\n",
        "    if idx < len(axes):\n",
        "        sns.barplot(x='label_display', y=feature, data=features_df, ax=axes[idx],\n",
        "                    palette='viridis', errorbar='sd', capsize=0.1) # 'sd' for standard deviation error bars\n",
        "\n",
        "        axes[idx].set_title(f'Mean {feature.replace(\"_\", \" \").title()} by Label', fontsize=12)\n",
        "        axes[idx].set_xlabel('Claim Label', fontsize=10)\n",
        "        axes[idx].set_ylabel(f'Mean {feature.replace(\"_\", \" \").title()}', fontsize=10)\n",
        "        axes[idx].tick_params(axis='x', rotation=30)\n",
        "        axes[idx].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Hide any unused subplots if features_to_plot is smaller than grid size\n",
        "for i in range(len(features_to_plot), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Average Linguistic Features Across Claim Labels', y=1.02, fontsize=16, fontweight='bold')\n",
        "plt.savefig('04_mean_features_bar_chart.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: 04_mean_features_bar_chart.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary Analysis"
      ],
      "metadata": {
        "id": "1WnLVaJXa31Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "for claim in train_df['claim_clean']:\n",
        "  words = [w.lower() for w in word_tokenize(claim) if w.isalpha()]\n",
        "  all_words.extend(words)\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "print(f\"Total words: {len(all_words):,}\")\n",
        "print(f\"Unique words: {len(word_counts):,}\")\n",
        "print(f\"Type-Token Ratio: {len(word_counts)/len(all_words):.4f}\")\n",
        "print(f\"\\nTop 15 words:\")\n",
        "for word, count in word_counts.most_common(15):\n",
        "  print(f\"{word}: {count:,}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IuWA3-Iqa7uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize top words\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "top_words_train = dict(word_counts.most_common(20))\n",
        "axes[0].barh(list(top_words_train.keys()), list(top_words_train.values()), color='#e74c3c')\n",
        "axes[0].set_title('Top 20 Words - Train')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Dev set\n",
        "all_words_dev = [w.lower() for claim in dev_df['claim_clean'] for w in word_tokenize(claim) if w.isalpha()]\n",
        "word_counts_dev = Counter(all_words_dev)\n",
        "top_words_dev = dict(word_counts_dev.most_common(20))\n",
        "axes[1].barh(list(top_words_dev.keys()), list(top_words_dev.values()), color='#3498db')\n",
        "axes[1].set_title('Top 20 Words - Dev')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('04_vocabulary.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wCb_4DSJguOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistical Analysis (SUPPORTED vs REFUTED)"
      ],
      "metadata": {
        "id": "4862MXsIg8DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "features_to_test =['num_adj','num_adv','num_modal','polarity','subjectivity']\n",
        "\n",
        "results = []\n",
        "for feature in features_to_test:\n",
        "  supported = features_df[features_df['label'] == 'SUPPORTS'][feature] # Filter by string label\n",
        "  refuted = features_df[features_df['label'] =='REFUTES'][feature] # Filter by string label\n",
        "\n",
        "  # Check if both series are not empty before performing t-test\n",
        "  if not supported.empty and not refuted.empty:\n",
        "    t_stat, p_value = stats.ttest_ind(supported, refuted, equal_var=False) # Use Welch's t-test if variances are unequal\n",
        "    diff_pct = ((refuted.mean() - supported.mean()) / supported.mean() * 100 ) if supported.mean() != 0 else 0\n",
        "\n",
        "    print(f\"\\n{feature}\")\n",
        "    print(f\" SUPPORTED: {supported.mean():.3f}\")\n",
        "    print(f\" REFUTED: {refuted.mean():.3f}\")\n",
        "    print(f\" Difference: {diff_pct:+.1f}%\")\n",
        "    print(f\" p-value: {p_value:.2e} {'Significant' if p_value < 0.05 else 'Not Significant'}\")\n",
        "\n",
        "    results.append({\n",
        "        'Feature': feature,\n",
        "        'SUPPORTED MEAN': supported.mean(),\n",
        "        'REFUTED MEAN': refuted.mean(),\n",
        "        'DIFFERENCE': diff_pct,\n",
        "        'p-value': p_value,\n",
        "        'Significant': 'Yes' if p_value <0.05 else 'No'\n",
        "    })\n",
        "  else:\n",
        "    print(f\"\\n{feature}: Not enough data for comparison (one or both labels are empty).\")\n",
        "\n",
        "stats_df = pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "4y3g726yhb3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv('train_processed.csv', index=False)\n",
        "dev_df.to_csv('dev_processed.csv', index=False)\n",
        "test_df.to_csv('test_processed.csv', index=False)\n",
        "features_df.to_csv('linguistic_features.csv', index=False)\n",
        "stats_df.to_csv('linguistic_stats.csv', index=False)"
      ],
      "metadata": {
        "id": "C_AgN8S7kGkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbstripout\n",
        "\n"
      ],
      "metadata": {
        "id": "TEg38bwrkQgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nbstripout NLP_Project.ipynb\n"
      ],
      "metadata": {
        "id": "CoaFTTiXkbbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}